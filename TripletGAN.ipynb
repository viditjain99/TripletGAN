{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "End_sem_assgn_triplet_gan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgpkFOPgoZiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n7OCZs64C6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd drive/My\\ Drive/DL/end_sem_assgn\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSZRu9V246Me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "from sklearn.metrics import accuracy_score,average_precision_score\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnaoeVvMUnIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logsumexp(x,dim=1):\n",
        "    m=torch.max(x,dim=dim)[0]\n",
        "    log_sum_exp=torch.log(torch.sum(torch.exp(x-m.unsqueeze(1)),dim=dim))+m\n",
        "    return log_sum_exp\n",
        "\n",
        "def supervised_loss(a,p,n):\n",
        "    dist_pos=torch.sqrt(torch.sum((a-p)**2,dim=1))\n",
        "    dist_neg=torch.sqrt(torch.sum((a-n)**2,dim=1))\n",
        "    dist_pos=torch.unsqueeze(dist_pos,-1)\n",
        "    dist_neg=torch.unsqueeze(dist_neg,-1)\n",
        "    dist_vec=torch.cat([dist_pos,dist_neg],dim=1)\n",
        "    loss=torch.mean(logsumexp(dist_vec,dim=1))-torch.mean(dist_neg)\n",
        "    return loss\n",
        "\n",
        "def unsupervised_loss(true,fake):\n",
        "    true_log_sum_exp=logsumexp(true)\n",
        "    true_softplus=F.softplus(logsumexp(true))\n",
        "    fake_softplus=F.softplus(logsumexp(fake))\n",
        "    loss=0.5*(torch.mean(true_softplus)+torch.mean(fake_softplus)-torch.mean(true_log_sum_exp))\n",
        "    return loss\n",
        "\n",
        "def discriminator_loss(a,p,n,true,fake):\n",
        "    sup_loss=supervised_loss(a,p,n)\n",
        "    unsup_loss=unsupervised_loss(true,fake)\n",
        "    loss=sup_loss+unsup_loss\n",
        "    return loss.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Zcj7puq8xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#implementation taken from: https://github.com/Sleepychord/ImprovedGAN-pytorch/blob/8aca3021edb771e2fc14bcbd409bd7f1cd453341/functional.py#L13\n",
        "class WeightNorm(torch.nn.Module):\n",
        "    def __init__(self,in_features,out_features,bias=True,weight_scale=None,weight_init_stdv=0.1):\n",
        "        super(WeightNorm,self).__init__()\n",
        "        self.in_features=in_features\n",
        "        self.out_features=out_features\n",
        "        self.weight=Parameter(torch.randn(out_features,in_features)*weight_init_stdv)\n",
        "        if bias:\n",
        "            self.bias=Parameter(torch.zeros(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias',None)\n",
        "        if weight_scale is not None:\n",
        "            assert type(weight_scale)==int\n",
        "            self.weight_scale=Parameter(torch.ones(out_features,1)*weight_scale)\n",
        "        else:\n",
        "            self.weight_scale=1 \n",
        "    def forward(self,x):\n",
        "        W = self.weight*self.weight_scale/torch.sqrt(torch.sum(self.weight**2,dim=1,keepdim=True))\n",
        "        return F.linear(x,W,self.bias)\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ +'('+'in_features='+str(self.in_features)+', out_features='+str(self.out_features)+')'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K74Hvnm3Hhuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self,training,noise_dim,output_dim=784):\n",
        "        super(Generator,self).__init__()\n",
        "        self.training=training\n",
        "        self.noise_dim=noise_dim\n",
        "\n",
        "        self.fc1=nn.Linear(noise_dim,500)\n",
        "        self.bn1=nn.BatchNorm1d(500,affine=False,eps=1e-6,momentum=0.5)\n",
        "        self.fc2=nn.Linear(500,500)\n",
        "        self.bn2=nn.BatchNorm1d(500,affine=False,eps=1e-6,momentum=0.5)\n",
        "        self.fc3=WeightNorm(500,output_dim,weight_scale=1)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self,batch_size):\n",
        "        noise=Variable(torch.rand(batch_size,self.noise_dim),requires_grad=False,volatile=not self.training)\n",
        "        noise=noise.to(device)\n",
        "\n",
        "        x=self.fc1(noise)\n",
        "        x=F.softplus(x)\n",
        "        x=self.bn1(x)\n",
        "\n",
        "        x=self.fc2(x)\n",
        "        x=F.softplus(x)\n",
        "        x=self.bn2(x)\n",
        "        \n",
        "        x=self.fc3(x)\n",
        "        x=torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self,output_dim,pretrain,training=True):\n",
        "        super(Discriminator,self).__init__()\n",
        "        self.input_dim=784\n",
        "        self.training=training\n",
        "        self.pretrain=pretrain\n",
        "\n",
        "        self.fc1=WeightNorm(self.input_dim,1000)\n",
        "        self.fc2=WeightNorm(1000,500)\n",
        "        self.fc3=WeightNorm(500,250)\n",
        "        self.fc4=WeightNorm(250,250)\n",
        "        self.fc5=WeightNorm(250,250)\n",
        "        self.fc6=WeightNorm(250,output_dim,weight_scale=1)\n",
        "        self.out=WeightNorm(output_dim,1,weight_scale=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=x.view(-1,self.input_dim)\n",
        "        if self.training:\n",
        "            noise=torch.randn(x.size())*0.3\n",
        "            noise=noise.to(device)\n",
        "            x=x+Variable(noise,requires_grad=False)\n",
        "        \n",
        "        x=self.fc1(x)\n",
        "        x=F.relu(x)\n",
        "        if self.training:\n",
        "            noise=torch.randn(x.size())*0.5\n",
        "            noise=noise.to(device)\n",
        "            x=x+Variable(noise,requires_grad=False)\n",
        "\n",
        "        x=self.fc2(x)\n",
        "        x=F.relu(x)\n",
        "        if self.training:\n",
        "            noise=torch.randn(x.size())*0.5\n",
        "            noise=noise.to(device)\n",
        "            x=x+Variable(noise,requires_grad=False)\n",
        "\n",
        "        x=self.fc3(x)\n",
        "        x=F.relu(x)\n",
        "        if self.training:\n",
        "            noise=torch.randn(x.size())*0.5\n",
        "            noise=noise.to(device)\n",
        "            x=x+Variable(noise,requires_grad=False)\n",
        "\n",
        "        x=self.fc4(x)\n",
        "        x=F.relu(x)\n",
        "        if self.training:\n",
        "            noise=torch.randn(x.size())*0.5\n",
        "            noise=noise.to(device)\n",
        "            x=x+Variable(noise,requires_grad=False)\n",
        "\n",
        "        x=self.fc5(x)\n",
        "        x1=F.relu(x)\n",
        "        if self.training:\n",
        "            noise=torch.randn(x.size())*0.5\n",
        "            noise=noise.to(device)\n",
        "            x=x+Variable(noise,requires_grad=False)\n",
        "\n",
        "        x=self.fc6(x)\n",
        "        if self.pretrain:\n",
        "            x=F.relu(x)\n",
        "            x=self.out(x)\n",
        "        return x,x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_WckHziCj5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set=datasets.MNIST(os.getcwd(),download=True,train=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma14nJ8zFZSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=train_set.data[:]\n",
        "y_train=train_set.targets[:]\n",
        "x_train=x_train.detach().numpy()\n",
        "y_train=y_train.detach().numpy()\n",
        "x_train=x_train/255.0\n",
        "print(x_train.shape,y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGJWzB27fuPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=64\n",
        "noise_dim=100\n",
        "output_dim=32\n",
        "learning_rate=0.003\n",
        "n_classes=10\n",
        "n_samples=10\n",
        "epochs=100\n",
        "img_size=28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yBzrvN8p2FC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_batches_train=len(x_train)//batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dfXz7S-jlPd",
        "colab_type": "text"
      },
      "source": [
        "Pre-train GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf-synrQp14j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator=Generator(training=True,noise_dim=noise_dim).to(device)\n",
        "discriminator=Discriminator(output_dim,pretrain=True,training=True).to(device)\n",
        "\n",
        "print(generator)\n",
        "total_params=sum(p.numel() for p in generator.parameters() if p.requires_grad)\n",
        "print('total_params:',total_params)\n",
        "print()\n",
        "print(discriminator)\n",
        "total_params=sum(p.numel() for p in discriminator.parameters() if p.requires_grad)\n",
        "print('total_params:',total_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpP_dx8Rqq6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_optim=torch.optim.Adam(generator.parameters(),lr=0.0003)\n",
        "dis_optim=torch.optim.Adam(discriminator.parameters(),lr=0.0003)\n",
        "gen_criterion=nn.BCEWithLogitsLoss()\n",
        "dis_criterion=nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px3SG1uskU96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_idx=list(range(0,x_train.shape[0]))\n",
        "shuffle(train_idx)\n",
        "x_train=x_train[train_idx]\n",
        "x_train=torch.from_numpy(x_train).type(torch.FloatTensor).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VweujhzfhAPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(10):\n",
        "    start_time=time.time()\n",
        "    epoch_d_loss=0\n",
        "    epoch_g_loss=0\n",
        "    for i in range(n_batches_train):\n",
        "        x_train_batch=x_train[i*batch_size:(i+1)*batch_size]\n",
        "\n",
        "        discriminator.zero_grad()\n",
        "        label=(torch.ones(batch_size)*0.9).to(device)\n",
        "        output,_=discriminator(x_train_batch)\n",
        "        output=output.view(-1)\n",
        "        d_loss_real=dis_criterion(output,label)\n",
        "        d_loss_real.backward()\n",
        "\n",
        "        fake_imgs_batch=generator(batch_size)\n",
        "        label=torch.zeros(batch_size).to(device)\n",
        "        output,_=discriminator(fake_imgs_batch.detach())\n",
        "        output=output.view(-1)\n",
        "        d_loss_fake=dis_criterion(output,label)\n",
        "        d_loss_fake.backward()\n",
        "        epoch_d_loss+=d_loss_real.item()+d_loss_fake.item()\n",
        "        dis_optim.step()\n",
        "\n",
        "        generator.zero_grad()\n",
        "        fake_imgs_batch=generator(batch_size)\n",
        "        label=(torch.ones(batch_size)*0.9).to(device)\n",
        "        output,_=discriminator(fake_imgs_batch)\n",
        "        output=output.view(-1)\n",
        "        g_loss=gen_criterion(output,label)\n",
        "        g_loss.backward()\n",
        "        epoch_g_loss+=g_loss.item()\n",
        "        gen_optim.step()\n",
        "\n",
        "    epoch_d_loss=epoch_d_loss/n_batches_train\n",
        "    epoch_g_loss=epoch_g_loss/n_batches_train\n",
        "    print('Epoch '+str(epoch+1)+'/'+str(10)+' epoch_duration: '+str(time.time()-start_time)+'s'+' g_loss: '+str(epoch_g_loss)+' d_loss: '+str(epoch_d_loss))\n",
        "    if (epoch+1)%5==0:\n",
        "        torch.save(generator.state_dict(),os.getcwd()+'/pre_train_generator_'+str(output_dim)+'.pt')\n",
        "        torch.save(discriminator.state_dict(),os.getcwd()+'/pre_train_discriminator_'+str(output_dim)+'.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOYhqfKOkkBL",
        "colab_type": "text"
      },
      "source": [
        "Pre-train end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "848tbn6OlW4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator=Generator(training=True,noise_dim=noise_dim).to(device)\n",
        "generator.load_state_dict(torch.load(os.getcwd()+'/pre_train_generator_'+str(output_dim)+'.pt'))\n",
        "discriminator=Discriminator(output_dim,pretrain=False,training=True).to(device)\n",
        "discriminator.load_state_dict(torch.load(os.getcwd()+'/pre_train_discriminator_'+str(output_dim)+'.pt'))\n",
        "\n",
        "print(generator)\n",
        "total_params=sum(p.numel() for p in generator.parameters() if p.requires_grad)\n",
        "print('total_params:',total_params)\n",
        "print()\n",
        "print(discriminator)\n",
        "total_params=sum(p.numel() for p in discriminator.parameters() if p.requires_grad)\n",
        "print('total_params:',total_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBmAAA2GlnY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_optim=torch.optim.Adam(generator.parameters(),lr=learning_rate,betas=(0.5,0.999))\n",
        "dis_optim=torch.optim.Adam(discriminator.parameters(),lr=learning_rate,betas=(0.5,0.999))\n",
        "gen_criterion=nn.MSELoss()\n",
        "dis_criterion=discriminator_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvFp0U-y1ylU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_1=copy.deepcopy(x_train)\n",
        "train_idx=list(range(0,x_train.shape[0]))\n",
        "shuffle(train_idx)\n",
        "x_train=x_train[train_idx]\n",
        "y_train=y_train[train_idx]\n",
        "shuffle(train_idx)\n",
        "x_train_1=x_train_1[train_idx]\n",
        "samples_x={}\n",
        "for i in range(n_classes):\n",
        "    if i not in samples_x.keys():\n",
        "        samples_x[i]=[]\n",
        "\n",
        "    samples_i=x_train[y_train==i]\n",
        "    shuffle(samples_i)\n",
        "    samples_x[i]=samples_i[:n_samples]\n",
        "\n",
        "x_train=torch.from_numpy(x_train).type(torch.FloatTensor).to(device)\n",
        "y_train=torch.from_numpy(y_train).to(device)\n",
        "x_train_1=torch.from_numpy(x_train_1).type(torch.FloatTensor).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB47SACopEli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g_loss_list=[]\n",
        "d_loss_list=[]\n",
        "for epoch in range(epochs):\n",
        "    start_time=time.time()\n",
        "    anchor=[]\n",
        "    pos=[]\n",
        "    neg=[]\n",
        "    labels=[]\n",
        "    epoch_g_loss=0\n",
        "    epoch_d_loss=0\n",
        "    for i in range(n_classes):\n",
        "        a=samples_x[i]\n",
        "        p=samples_x[i]\n",
        "        n=[]\n",
        "        for j in range(n_classes):\n",
        "            if j!=i:\n",
        "                samples=samples_x[j]\n",
        "                for k in range(len(samples)):\n",
        "                    n.append(samples[k])\n",
        "        n=np.array(n)\n",
        "        for j in range(a.shape[0]):\n",
        "            for k in range(n_classes*10-10):\n",
        "                n_idx=list(range(0,n.shape[0]))\n",
        "                shuffle(n_idx)\n",
        "                n_idx=n_idx[:n_samples]\n",
        "                n=n[n_idx]\n",
        "                anchor.append(a)\n",
        "                idx=list(range(0,n_samples))\n",
        "                shuffle(idx)\n",
        "                pos.append(p[idx])\n",
        "                neg.append(n[idx])\n",
        "\n",
        "    anchor=np.concatenate(anchor,axis=0)\n",
        "    pos=np.concatenate(pos,axis=0)\n",
        "    neg=np.concatenate(neg,axis=0)\n",
        "    train_idx=list(range(0,anchor.shape[0]))\n",
        "    shuffle(train_idx)\n",
        "    train_idx=train_idx[0:x_train.shape[0]]\n",
        "    anchor=anchor[train_idx]\n",
        "    pos=pos[train_idx]\n",
        "    neg=neg[train_idx]\n",
        "\n",
        "    anchor=torch.from_numpy(anchor).type(torch.FloatTensor).to(device)\n",
        "    pos=torch.from_numpy(pos).type(torch.FloatTensor).to(device)\n",
        "    neg=torch.from_numpy(neg).type(torch.FloatTensor).to(device)\n",
        "    \n",
        "    for i in range(n_batches_train):\n",
        "        anchor_batch=anchor[i*batch_size:(i+1)*batch_size]\n",
        "        pos_batch=pos[i*batch_size:(i+1)*batch_size]\n",
        "        neg_batch=neg[i*batch_size:(i+1)*batch_size]\n",
        "        x_train_batch=x_train[i*batch_size:(i+1)*batch_size]\n",
        "        x_train_1_batch=x_train_1[i*batch_size:(i+1)*batch_size]\n",
        "\n",
        "        fake_imgs_batch=generator(batch_size)\n",
        "\n",
        "        anchor_batch_net,_=discriminator(anchor_batch)\n",
        "        pos_batch_net,_=discriminator(pos_batch)\n",
        "        neg_batch_net,_=discriminator(neg_batch)\n",
        "        real_batch_net,_=discriminator(x_train_batch)\n",
        "        fake_batch_net,_=discriminator(fake_imgs_batch)\n",
        "        d_loss=dis_criterion(anchor_batch_net,pos_batch_net,neg_batch_net,real_batch_net,fake_batch_net)\n",
        "        dis_optim.zero_grad()\n",
        "        d_loss.backward()\n",
        "        dis_optim.step()\n",
        "        epoch_d_loss+=d_loss.item()\n",
        "\n",
        "        fake_imgs_batch=generator(batch_size)\n",
        "        _,fake_imgs_batch_net=discriminator(fake_imgs_batch)\n",
        "        _,x_train_1_batch_net=discriminator(x_train_1_batch)\n",
        "        g_loss=gen_criterion(fake_imgs_batch_net,x_train_1_batch_net)\n",
        "        gen_optim.zero_grad()\n",
        "        dis_optim.zero_grad()\n",
        "        g_loss.backward()\n",
        "        gen_optim.step()\n",
        "        epoch_g_loss+=g_loss.item()\n",
        "    \n",
        "    epoch_d_loss=epoch_d_loss/n_batches_train\n",
        "    epoch_g_loss=epoch_g_loss/n_batches_train\n",
        "    print('Epoch '+str(epoch+1)+'/'+str(epochs)+' epoch_duration: '+str(time.time()-start_time)+'s'+' g_loss: '+str(epoch_g_loss)+' d_loss: '+str(epoch_d_loss))\n",
        "    g_loss_list.append(epoch_g_loss)\n",
        "    d_loss_list.append(epoch_d_loss)\n",
        "    if (epoch+1)%5==0:\n",
        "        torch.save(generator.state_dict(),os.getcwd()+'/generator_'+str(n_samples*n_classes)+'_'+str(output_dim)+'.pt')\n",
        "        torch.save(discriminator.state_dict(),os.getcwd()+'/discriminator_'+str(n_samples*n_classes)+'_'+str(output_dim)+'.pt')\n",
        "        with h5py.File('metrics_list_'+str(n_samples*n_classes)+'_'+str(output_dim)+'.h5','w') as out:\n",
        "            out.create_dataset(\"generator\",data=np.array(g_loss_list))\n",
        "            out.create_dataset(\"discriminator\",data=np.array(d_loss_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOxYe71lZUEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics=h5py.File('metrics_list_'+str(n_samples*n_classes)+'_'+str(output_dim)+'.h5','r')\n",
        "g_loss_list=metrics['generator']\n",
        "d_loss_list=metrics['discriminator']\n",
        "g_loss_list=np.array(g_loss_list).tolist()\n",
        "d_loss_list=np.array(d_loss_list).tolist()\n",
        "metrics.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G69VHMT8VmAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('gen_loss vs epochs')\n",
        "plt.plot(g_loss_list)\n",
        "\n",
        "plt.figure()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('dis_loss vs epochs')\n",
        "plt.plot(d_loss_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9blF178a6QX",
        "colab_type": "text"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kueMFYrklRAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator=Discriminator(output_dim,pretrain=False,training=False)\n",
        "discriminator.load_state_dict(torch.load(os.getcwd()+'/discriminator_'+str(n_samples*n_classes)+'_'+str(output_dim)+'.pt',map_location=device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX2DtZVKrTqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator=Generator(training=False,noise_dim=noise_dim)\n",
        "generator=generator.to(device)\n",
        "generator.load_state_dict(torch.load(os.getcwd()+'/generator_'+str(n_samples*n_classes)+'_'+str(output_dim)+'.pt',map_location=device))\n",
        "with torch.no_grad():\n",
        "    images=generator(64)\n",
        "images=images.detach().cpu().numpy()\n",
        "fig,ax=plt.subplots(8,8)\n",
        "count=0\n",
        "for i in range(8):\n",
        "    for j in range(8):\n",
        "        img=images[count]\n",
        "        img=img.reshape((28,28))\n",
        "        ax[i,j].imshow(img,cmap='gray')\n",
        "        count+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmSEdMWfle1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=train_set.data[:]\n",
        "y_train=train_set.targets[:]\n",
        "x_train=x_train/255.0\n",
        "print(x_train.shape,y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wC7yGqWlzSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator.eval()\n",
        "batch_size=50\n",
        "features_train=np.empty((batch_size,output_dim))\n",
        "for i in range(len(x_train)//batch_size):\n",
        "    input=x_train[i*batch_size:(i+1)*batch_size]\n",
        "    features_pred,_=discriminator(input)\n",
        "    if i==0:\n",
        "        features_train=features_pred[:,:].detach().cpu().numpy()\n",
        "    else:\n",
        "        features_train=np.concatenate((features_train,features_pred.detach().cpu().numpy()),axis=0)\n",
        "\n",
        "print(features_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWTtsHoqqJhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn=KNN(n_neighbors=9)\n",
        "knn.fit(features_train,y_train.detach().cpu().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmYW-HqyXPjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set=datasets.MNIST(os.getcwd(),download=True,train=False)\n",
        "x_test=test_set.data[:]\n",
        "y_test=test_set.targets[:]\n",
        "x_test=x_test/255.0\n",
        "print(x_test.shape,y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsvIZUiKTT0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_test=np.empty((batch_size,output_dim))\n",
        "for i in range(len(x_test)//batch_size):\n",
        "    input=x_test[i*batch_size:(i+1)*batch_size]\n",
        "    features_pred,_=discriminator(input)\n",
        "    if i==0:\n",
        "        features_test=features_pred[:,:].detach().cpu().numpy()\n",
        "    else:\n",
        "        features_test=np.concatenate((features_test,features_pred.detach().cpu().numpy()),axis=0)\n",
        "\n",
        "print(features_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykcmIOewxNv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test=y_test.detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjWexfozqiht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mAP=0\n",
        "pred=knn.predict(features_test)\n",
        "acc=accuracy_score(y_test,pred)\n",
        "for i in range(n_classes):\n",
        "    y1=copy.deepcopy(pred)\n",
        "    y2=copy.deepcopy(y_test)\n",
        "    for j in range(y_test.shape[0]):\n",
        "        if y2[j]==i:\n",
        "            y2[j]=1\n",
        "        else:\n",
        "            y2[j]=0\n",
        "        \n",
        "        if y1[j]==i:\n",
        "            y1[j]=1\n",
        "        else:\n",
        "            y1[j]=0\n",
        "\n",
        "    ap=average_precision_score(y2,y1)\n",
        "    mAP+=ap\n",
        "    \n",
        "print('Accuracy: '+str(acc*100)+'%')\n",
        "print('mAP: '+str(mAP/n_classes))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}